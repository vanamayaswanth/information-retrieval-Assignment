{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write a program implemnt text preprocessing of the given text \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\" @My name is Yaswanth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "def clean_text(text):\n",
    "    text=regex.sub(r'[^\\w\\s]','',text)\n",
    "    text=regex.sub(r'\\s+',' ',text)\n",
    "    text=regex.sub(r'@+','',text)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' My name is Yaswanth'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text=clean_text(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'test', 'lemmatizer', ',', 'by', '191b278']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "text=\"This is a test lemmatizer, by 191b278\"\n",
    "sent_words=word_tokenize(text)\n",
    "sent_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'test', 'stemming', ',', 'by', '191b278']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer=PorterStemmer()\n",
    "text=\"This is a test stemming, by 191b278\"\n",
    "sent_words=word_tokenize(text)\n",
    "sent_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'test', 'tokenization', ',', 'by', '191b278']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text=\"This is a test tokenization, by 191b278\"\n",
    "sent_words=word_tokenize(text)\n",
    "sent_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inverted index\n",
    "def create_index(text):\n",
    "    index={}\n",
    "    words=word_tokenize(text)\n",
    "    for word in words:\n",
    "        if word not in index:\n",
    "            index[word]=1\n",
    "        else:\n",
    "            index[word]+=1\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'This': 1,\n",
       " 'is': 1,\n",
       " 'a': 1,\n",
       " 'test': 1,\n",
       " 'tokenization': 1,\n",
       " ',': 1,\n",
       " 'by': 1,\n",
       " '191b278': 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_index(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('My', 2), ('name', 1), ('is', 2), ('yaswanth', 1), ('and', 1), ('this', 1), ('work', 1), ('by', 1), ('191b278,', 1), ('191b278', 2), ('IR,', 1)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text = \"My My name is yaswanth and this work is by 191b278, 191b278 IR, 191b278\"\n",
    "tokens = [t for t in text.split()]\n",
    "freqs = nltk.FreqDist(tokens)\n",
    "blah_list = [(k, v) for k, v in freqs.items()]\n",
    "print(blah_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the term frequency and document frequency\n",
    "def tf(word,text):\n",
    "    count=0\n",
    "    for w in word:\n",
    "        if w==word:\n",
    "            count+=1\n",
    "    return count/len(word)\n",
    "text=\"My My name is yaswanth and this work is by 191b278, 191b278 IR, 191b278\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf(\"5\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the inverse document frequency and term frequency\n",
    "def idf(word,text):\n",
    "    count=0\n",
    "    for w in word:\n",
    "        if w==word:\n",
    "            count+=1\n",
    "    return count/len(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf(\"191b278\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text vectroization using tf-idf\n",
    "def tfidf(word,text):\n",
    "    return tf(word,text)*idf(word,text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document ranking using vector space model\n",
    "def rank(text):\n",
    "    index=vsm(text)\n",
    "    return index\n",
    "def vsm(text):\n",
    "    index=create_index(text)\n",
    "    return index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write a Program to classify whether the given SMS is Spam or Ham using Naive Bayes Classifier\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# gnb= GaussianNB()\n",
    "# y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a program to create multiple documents that contain normal string +and convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def tfidf(text):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(text)\n",
    "    return tfidf_matrix\n",
    "text = \"My name is yaswanth and this work is by 191b278, 191b278 IR, 191b278\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x10 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 10 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix = tfidf(text)\n",
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=1, random_state=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Write a program to convert multiple text documents into dusters based on type using K-Means clustering(use Tf- idf)\n",
    "from sklearn.cluster import KMeans\n",
    "def kmeans(text):\n",
    "    kmeans = KMeans(n_clusters=1, random_state=0).fit(tfidf_matrix)\n",
    "    return kmeans\n",
    "kmeans(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a program to convert multiple text documents into cluster documents by topics using K-Means Clustering (use bag-of-words approach for feature Extraction)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def bag_of_words(text):\n",
    "    count_vect = CountVectorizer()\n",
    "    X_train_counts = count_vect.fit_transform(text)\n",
    "    return X_train_counts\n",
    "bag_of_words(text)\n",
    "\n",
    "def kmeans(text):\n",
    "    kmeans = KMeans(n_clusters=1, random_state=0).fit(X_train_counts)\n",
    "    return kmeans\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a program to convert multiple text documents into cluster documents by topics using K- Nearest Neighbor Clustering (use bag-of-words approach for featureExtraction)\n",
    "\n",
    "#bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def bag_of_words(text):\n",
    "    count_vect = CountVectorizer()\n",
    "    X_train_counts = count_vect.fit_transform(text)\n",
    "    return X_train_counts\n",
    "bag_of_words(text)\n",
    "\n",
    "\n",
    "def knn(text):\n",
    "    knn = KNeighborsClassifier(n_neighbors=1)\n",
    "    knn.fit(X_train_counts, y_train)\n",
    "    return knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " google\n",
      " cat\n",
      " best\n",
      " climbing\n",
      " ninja\n",
      " translate\n",
      " app\n",
      " incredible\n",
      " promoter\n",
      " chrome\n",
      "Cluster 1:\n",
      " impressed\n",
      " map\n",
      " feedback\n",
      " google\n",
      " ve\n",
      " eating\n",
      " face\n",
      " extension\n",
      " climbing\n",
      " key\n",
      "\n",
      "\n",
      "Prediction\n",
      "[0]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "#Write a program to convert long list of strings (words) in a document into clusters using K-Means Clustering.\n",
    "#tokenize the text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "documents = [\"This little kitty came to play when I was eating at a restaurant.\",\n",
    "             \"Merley has the best squooshy kitten belly.\",\n",
    "             \"Google Translate app is incredible.\",\n",
    "             \"If you open 100 tab in google you get a smiley face.\",\n",
    "             \"Best cat photo I've ever taken.\",\n",
    "             \"Climbing ninja cat.\",\n",
    "             \"Impressed with google map feedback.\",\n",
    "             \"Key promoter extension for Google Chrome.\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "true_k = 2\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    print\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Prediction\")\n",
    "\n",
    "Y = vectorizer.transform([\"chrome browser to open.\"])\n",
    "prediction = model.predict(Y)\n",
    "print(prediction)\n",
    "\n",
    "Y = vectorizer.transform([\"My cat is hungry.\"])\n",
    "prediction = model.predict(Y)\n",
    "print(prediction)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#web crawling using Breadth First Search\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "\n",
    "def get_links(url):\n",
    "    html_text = requests.get(url).text\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "    links = []\n",
    "    for link in soup.find_all('a'):\n",
    "        links.append(link.get('href'))\n",
    "    return links\n",
    "\n",
    "\n",
    "def get_text(url):\n",
    "    html_text = requests.get(url).text\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_urls(url):\n",
    "    links = get_links(url)\n",
    "    urls = []\n",
    "    for link in links:\n",
    "        if re.match(\"^http\", link):\n",
    "            urls.append(link)\n",
    "    return urls\n",
    "\n",
    "\n",
    "def get_urls_text(url):\n",
    "    urls = get_urls(url)\n",
    "    texts = []\n",
    "    for url in urls:\n",
    "        texts.append(get_text(url))\n",
    "    return texts\n",
    "\n",
    "\n",
    "def get_urls_links(url):\n",
    "    urls = get_urls(url)\n",
    "    links = []\n",
    "    for url in urls:\n",
    "        links.append(get_links(url))\n",
    "    return links\n",
    "\n",
    "\n",
    "def level_crawel(url, level):\n",
    "    if level == 0:\n",
    "        return\n",
    "    urls = get_urls(url)\n",
    "    for url in urls:\n",
    "        level_crawel(url, level - 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intern - https://www.marktechpost.com/category/technology/artificial-intelligence/natural-language-processing/\n",
      "Intern - https://www.marktechpost.com/category/technology/artificial-intelligence/machine-learning/\n",
      "Intern - https://www.marktechpost.com/category/technology/artificial-intelligence/deep-learning-artificial-intelligence/\n",
      "Intern - https://www.marktechpost.com/category/technology/ai-shorts/\n",
      "Intern - https://www.marktechpost.com/category/technology/artificial-intelligence/computer-vision/\n",
      "Intern - https://www.marktechpost.com/category/tech-news/federated-learning/\n",
      "Intern - https://www.marktechpost.com/category/technology/reinforcement-learning/\n",
      "Intern - https://www.marktechpost.com/category/technology/artificial-intelligence/automl/\n",
      "Intern - https://www.marktechpost.com/category/tech-news/startups/\n",
      "Intern - https://www.marktechpost.com/category/interview/\n",
      "Intern - https://www.marktechpost.com/free-ai-introductory-course-for-all/\n",
      "Intern - https://www.marktechpost.com/introduction-to-machine-learning-with-python-course-free/\n",
      "Intern - https://www.marktechpost.com/python-for-machine-learning-ml-free-course/\n",
      "Intern - https://www.marktechpost.com/mathematics-for-machine-learning-course-free/\n",
      "Intern - https://www.marktechpost.com/free-resources/\n",
      "Intern - https://www.marktechpost.com/ai-paper-summary/\n",
      "Intern - https://www.marktechpost.com/about-us/\n",
      "Intern - https://www.marktechpost.com/advisory-board-members/\n",
      "Intern - https://www.marktechpost.com/advertisement/\n",
      "Intern - https://www.marktechpost.com/\n",
      "Extern - https://bit.ly/38BS5pM\n",
      "Intern - https://www.marktechpost.com/category/technology/artificial-intelligence/\n",
      "Intern - https://www.marktechpost.com/2022/05/31/microsoft-introduce-adatest-a-process-for-adaptive-testing-and-debugging-of-nlp-models-inspired-by-the-test-debug-cycle-in-traditional-software-engineering/\n",
      "Intern - https://www.marktechpost.com/author/khushboo-gupta/\n",
      "Intern - https://www.marktechpost.com/2022/05/28/coauthor-a-human-ai-collaborative-writing-dataset-for-improving-language-tools/\n",
      "Intern - https://www.marktechpost.com/author/aneesh-tickoo/\n",
      "Intern - https://www.marktechpost.com/2022/05/26/microsoft-ai-researchers-introduces-detoxigen-a-large-scale-machine-generated-dataset-for-adversarial-and-implicit-hate-speech-detection/\n",
      "Intern - https://www.marktechpost.com/2022/05/18/google-researchers-explain-the-rephrasing-in-google-assistant-based-on-context/\n",
      "Intern - https://www.marktechpost.com/author/annu-kumari/\n",
      "Intern - https://www.marktechpost.com/2022/05/17/tensorflow-introduces-a-new-on-device-embedding-based-search-library-that-allows-find-similar-images-text-or-audio-from-millions-of-data-samples-in-a-few-milliseconds/\n",
      "Intern - https://www.marktechpost.com/2022/05/14/this-germany-based-ai-startup-is-developing-the-next-enterprise-search-engine-fueled-by-nlp-and-open-source/\n",
      "Intern - https://www.marktechpost.com/author/mansirawat/\n",
      "Intern - https://www.marktechpost.com/cdn-cgi/l/email-protection\n",
      "Intern - https://www.marktechpost.com/2022/04/24/amazon-kickstarts-natural-language-understanding-by-open-sourcing-massive-speech-dataset/\n",
      "Intern - https://www.marktechpost.com/2022/04/19/meet-lovo-a-berkeley-based-next-generation-ai-voiceover-startup/\n",
      "Intern - https://www.marktechpost.com/2022/04/15/salesforce-ai-researchers-introduce-converse-a-task-oriented-dialogue-system-that-simplifies-chatbot-building-and-handles-complex-tasks/\n",
      "Intern - https://www.marktechpost.com/author/tanushree/\n",
      "Intern - https://www.marktechpost.com/2022/04/11/now-you-can-search-on-google-with-text-and-images-at-the-same-time-using-googles-new-ai-powered-multisearch-feature/\n",
      "Intern - https://www.marktechpost.com/author/shrutigarg11/\n",
      "Intern - https://www.marktechpost.com/category/technology/artificial-intelligence/natural-language-processing/page/2/\n",
      "Intern - https://www.marktechpost.com/category/technology/artificial-intelligence/natural-language-processing/page/3/\n",
      "Intern - https://www.marktechpost.com/category/technology/artificial-intelligence/natural-language-processing/page/16/\n",
      "Extern - https://www.facebook.com/MarkTechPost/\n",
      "Extern - https://twitter.com/Marktechpost\n",
      "Extern - https://www.youtube.com/channel/UCBBh7FjO1M-t4ZRxm0F0UBw\n",
      "Intern - https://www.marktechpost.com/download/\n",
      "Intern - https://www.marktechpost.com/ai-magazine/\n",
      "Intern - https://www.marktechpost.com/privacy-policy/\n",
      "Intern - https://www.marktechpost.com/cookie-policy/\n",
      "Intern - https://www.marktechpost.com/industry-news/\n",
      "Intern - https://www.marktechpost.com/2022/06/02/ai-researchers-from-priorai2-release-grit-a-general-robust-image-task-benchmark-for-evaluating-computer-vision-models-performace/\n",
      "Intern - https://www.marktechpost.com/category/tech-news/ai-paper-summary/\n",
      "Intern - https://www.marktechpost.com/2022/06/01/nvidias-cambridge-1-supercomputer-and-monai-were-utilized-by-researchers-at-kings-college-london-to-develop-open-source-synthetic-brain-pictures-which-will-help-to-speed-up-ai-in-healthcare/\n",
      "Intern - https://www.marktechpost.com/2022/06/01/in-the-latest-ai-research-deepmind-researchers-propose-steps-for-scaling-implicit-neural-representations-inrs/\n",
      "Extern - https://bit.ly/3Lc1uSQ\n",
      "Extern - javascript://void(0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from urllib.request import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.request import urlparse\n",
    "\n",
    "links_intern = set()\n",
    "input_url = \"https://www.marktechpost.com/category/technology/artificial-intelligence/natural-language-processing/\"\n",
    "depth = 1\n",
    "links_extern = set()\n",
    "\n",
    "def level_crawler(input_url):\n",
    "\ttemp_urls = set()\n",
    "\tcurrent_url_domain = urlparse(input_url).netloc\n",
    "\n",
    "\t\n",
    "\tbeautiful_soup_object = BeautifulSoup(\n",
    "\t\trequests.get(input_url).content, \"lxml\")\n",
    "\n",
    "\t\n",
    "\tfor anchor in beautiful_soup_object.findAll(\"a\"):\n",
    "\t\thref = anchor.attrs.get(\"href\")\n",
    "\t\tif(href != \"\" or href != None):\n",
    "\t\t\thref = urljoin(input_url, href)\n",
    "\t\t\thref_parsed = urlparse(href)\n",
    "\t\t\thref = href_parsed.scheme\n",
    "\t\t\thref += \"://\"\n",
    "\t\t\thref += href_parsed.netloc\n",
    "\t\t\thref += href_parsed.path\n",
    "\t\t\tfinal_parsed_href = urlparse(href)\n",
    "\t\t\tis_valid = bool(final_parsed_href.scheme) and bool(\n",
    "\t\t\t\tfinal_parsed_href.netloc)\n",
    "\t\t\tif is_valid:\n",
    "\t\t\t\tif current_url_domain not in href and href not in links_extern:\n",
    "\t\t\t\t\tprint(\"Extern - {}\".format(href))\n",
    "\t\t\t\t\tlinks_extern.add(href)\n",
    "\t\t\t\tif current_url_domain in href and href not in links_intern:\n",
    "\t\t\t\t\tprint(\"Intern - {}\".format(href))\n",
    "\t\t\t\t\tlinks_intern.add(href)\n",
    "\t\t\t\t\ttemp_urls.add(href)\n",
    "\treturn temp_urls\n",
    "\n",
    "\n",
    "if(depth == 0):\n",
    "\tprint(\"Intern - {}\".format(input_url))\n",
    "\n",
    "elif(depth == 1):\n",
    "\tlevel_crawler(input_url)\n",
    "\n",
    "else:\n",
    "\t\n",
    "\tqueue = []\n",
    "\tqueue.append(input_url)\n",
    "\tfor j in range(depth):\n",
    "\t\tfor count in range(len(queue)):\n",
    "\t\t\turl = queue.pop(0)\n",
    "\t\t\turls = level_crawler(url)\n",
    "\t\t\tfor i in urls:\n",
    "\t\t\t\tqueue.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "How Does the Internet Work?\n",
      "                \n",
      "\n",
      "2\n",
      "8 Reasons Why You Should Pick TypeScript Over JavaScript\n",
      "                \n",
      "\n",
      "3Node having maximum sum of immediate children and itself in n-ary tree\n",
      "4Get level of a node in binary tree  Iterative approach\n",
      "5Print extreme nodes of each level of Binary Tree in alternate order\n",
      "6Convert a given Binary tree to a tree that holds Logical AND property\n",
      "7Find depth of the deepest odd level leaf node\n",
      "8Find sum of all right leaves in a given Binary Tree\n",
      "9Vertical width of Binary tree | Set 2\n",
      "10Print the nodes at odd levels of a tree\n",
      "11Modify a binary tree to get preorder traversal using right pointers only\n",
      "12Replace node with depth in a binary tree\n",
      "13\n",
      "Count of Subarrays which Contain the Length of that Subarray\n",
      "                \n",
      "\n",
      "14\n",
      "Count ordered pairs of Array elements such that bitwise AND of K and XOR of the pair is 0\n",
      "                \n",
      "\n",
      "15\n",
      "Lexicographically smallest String by moving one Subsequence to the end\n",
      "                \n",
      "\n",
      "31Perfect Binary Tree Specific Level Order Traversal | Set 2\n",
      "32Check if each internal node of a BST has exactly one child\n",
      "33Count BST nodes that lie in a given range\n",
      "34Right view of Binary Tree using Queue\n",
      "35Remove all the half nodes of a given Binary Tree\n",
      "36Find sum of all left leaves in a given Binary Tree\n",
      "37Kth ancestor of a node in binary tree | Set 2\n",
      "38Sorted order printing of a given array that represents a BST\n",
      "39Print Binary Tree levels in sorted order\n",
      "40Find n-th node of inorder traversal\n",
      "41How to Create a Tensor Whose Elements are Sampled from a Poisson Distribution in PyTorch\n",
      "42How to Find Mean Across the Image Channels in PyTorch?\n",
      "43How to Import Kaggle Datasets Directly into Google Colab\n",
      "44How to Download Kaggle Datasets into Jupyter Notebook\n",
      "45How to Create a Normal Distribution in Python PyTorch\n",
      "46Check whether a given binary tree is perfect or not\n",
      "47Find Count of Single Valued Subtrees\n",
      "48Root to leaf path with maximum distinct nodes\n",
      "49Program to Delete a Tree\n",
      "50Extract Leaves of a Binary Tree in a Doubly Linked List\n",
      "51Averages of Levels in Binary Tree\n",
      "52Populate Inorder Successor for all nodes\n",
      "53Print Binary Tree in 2-Dimensions\n",
      "54Inorder Non-threaded Binary Tree Traversal without Recursion or Stack\n",
      "55Find the node with minimum value in a Binary Search Tree\n",
      "56Factors Affecting Agriculture In India\n",
      "57Conquest And Expansion of Samudragupta\n",
      "58Why Fundamental Duties Are Not Enforceable By Law?\n",
      "59Role Of Rice In Boosting Nourishment\n",
      "60Role and Functions of NGOs\n",
      "61Java Program to Check whether a Char is a Vowel or Consonant\n",
      "62Check whether a given binary tree is perfect or not\n",
      "63Find Count of Single Valued Subtrees\n",
      "64Root to leaf path with maximum distinct nodes\n",
      "65Program to Delete a Tree\n",
      "66Extract Leaves of a Binary Tree in a Doubly Linked List\n",
      "67Averages of Levels in Binary Tree\n",
      "68Populate Inorder Successor for all nodes\n",
      "69Print Binary Tree in 2-Dimensions\n",
      "70Inorder Non-threaded Binary Tree Traversal without Recursion or Stack\n",
      "71Which property is used to control the scrolling of an image in the background ?\n",
      "72jQWidgets jqxKanban getItems() Method\n",
      "73jQWidgets jqxKanban getColumnItems() Method\n",
      "74jQuery UI Selectmenu instance() Method\n",
      "75jQWidgets jqxKanban getColumn() Method\n",
      "76Sum of nodes at maximum depth of a Binary Tree\n",
      "77Remove all nodes which don’t lie in any path with sum greater than or equal to k\n",
      "78Number of siblings of a given Node in n-ary Tree\n",
      "79Maximum Consecutive Increasing Path Length in Binary Tree\n",
      "80Largest number in BST which is less than or equal to N\n",
      "81Find next right node of a given key\n",
      "82Check if all leaves are at same level\n",
      "83TCS NQT Programming MCQ Questions and Answers\n",
      "84Level order traversal line by line | Set 3 (Using One Queue)\n",
      "85Density of Binary Tree in One Traversal\n",
      "86Difference Between Owncloud VS Nextcloud\n",
      "87How to Disable Bluetooth in Ubuntu\n",
      "88Create Desktop Shortcuts Using The Terminal on Ubuntu\n",
      "89SQLiv – Massive SQL Injection Scanner\n",
      "90TryHackMe – Passive Reconnaissance Solution\n",
      "91Node having maximum sum of immediate children and itself in n-ary tree\n",
      "92Get level of a node in binary tree Iterative approach\n",
      "93Print extreme nodes of each level of Binary Tree in alternate order\n",
      "94Convert a given Binary tree to a tree that holds Logical AND property\n",
      "95Find depth of the deepest odd level leaf node\n",
      "96Find sum of all right leaves in a given Binary Tree\n",
      "97Vertical width of Binary tree | Set 2\n",
      "98Print the nodes at odd levels of a tree\n",
      "99Modify a binary tree to get preorder traversal using right pointers only\n",
      "100Replace node with depth in a binary tree\n",
      "101How to Install Turtle in Python on Linux?\n",
      "102How to install pgadmin4 in Kali Linux\n",
      "103How to Create Bottom Navigation using Kivymd and Python\n",
      "104all.equal() Function in R\n",
      "105Convert NA into Factor Level in R\n",
      "106Node having maximum sum of immediate children and itself in n-ary tree\n",
      "107Get level of a node in binary tree Iterative approach\n",
      "108Print extreme nodes of each level of Binary Tree in alternate order\n",
      "109Convert a given Binary tree to a tree that holds Logical AND property\n",
      "110Find depth of the deepest odd level leaf node\n",
      "111Find sum of all right leaves in a given Binary Tree\n",
      "112Vertical width of Binary tree | Set 2\n",
      "113Print the nodes at odd levels of a tree\n",
      "114Modify a binary tree to get preorder traversal using right pointers only\n",
      "115Replace node with depth in a binary tree\n",
      "116How to Measure the Binary Cross Entropy Between the Target and the Input Probabilities in PyTorch?\n",
      "117Use Previous Row of data.table in R\n",
      "118Lower Tail Test of Population Proportion in R\n",
      "119Upper Tail Test of Population Proportion in R\n",
      "120Create a Scatter Plot using Sepal length and Petal_width to Separate the Species Classes Using scikit-learn\n",
      "121Level order traversal line by line | Set 3 (Using One Queue)\n",
      "122Density of Binary Tree in One Traversal\n",
      "123Tilt of Binary Tree\n",
      "124Convert an arbitrary Binary Tree to a tree that holds Children Sum Property\n",
      "125Vertical width of Binary tree | Set 1\n",
      "126Sum of k smallest elements in BST\n",
      "127Perfect Binary Tree Specific Level Order Traversal | Set 2\n",
      "128Check if each internal node of a BST has exactly one child\n",
      "129Count BST nodes that lie in a given range\n",
      "130Right view of Binary Tree using Queue\n",
      "131How to Create Frequency Table by Group using Dplyr in R\n",
      "132Fill Matrix With Loop in R\n",
      "133Draw ggplot2 Barplot With Round Corners in R\n",
      "134ggvenn Package in R\n",
      "135as.double() and is.double() Functions in R\n",
      "136Node having maximum sum of immediate children and itself in n-ary tree\n",
      "137Get level of a node in binary tree Iterative approach\n",
      "138Print extreme nodes of each level of Binary Tree in alternate order\n",
      "139Convert a given Binary tree to a tree that holds Logical AND property\n",
      "140Find depth of the deepest odd level leaf node\n",
      "141Find sum of all right leaves in a given Binary Tree\n",
      "142Vertical width of Binary tree | Set 2\n",
      "143Print the nodes at odd levels of a tree\n",
      "144Modify a binary tree to get preorder traversal using right pointers only\n",
      "145Replace node with depth in a binary tree\n",
      "146P_load() function in R\n",
      "147hasName() Function in R\n",
      "148Interactive() Function in R\n",
      "149str_c() Function of stringr Package in R\n",
      "150Write Data Into Excel Using R\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "URL = 'https://www.geeksforgeeks.org/page/'\n",
    "\n",
    "for page in range(1,10):\n",
    "\treq = requests.get(URL + str(page) + '/')\n",
    "\tsoup = bs(req.text, 'html.parser')\n",
    "\n",
    "\ttitles = soup.find_all('div',attrs={'class','head'})\n",
    "\n",
    "\tfor i in range(4,19):\n",
    "\t\tif page>1:\n",
    "\t\t\tprint(f\"{(i-3)+page*15}\" + titles[i].text)\n",
    "\t\telse:\n",
    "\t\t\tprint(f\"{i-3}\" + titles[i].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a program to crawl a given web page and get most frequent words.\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "def start_crawl(url):\n",
    "    html_text = requests.get(url).text\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    return text\n",
    "\n",
    "def get_words(text):\n",
    "    words = re.findall(r'\\w+', text)\n",
    "    return words\n",
    "\n",
    "def get_freq(words):\n",
    "    freq = {}\n",
    "    for word in words:\n",
    "        if word in freq:\n",
    "            freq[word] += 1\n",
    "        else:\n",
    "            freq[word] = 1\n",
    "    return freq\n",
    "\n",
    "url= \"https://www.marktechpost.com/category/technology/artificial-intelligence/natural-language-processing/\"\n",
    "text = start_crawl(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Natural': 8,\n",
       " 'Language': 8,\n",
       " 'Processing': 6,\n",
       " 'Category': 1,\n",
       " 'MarkTechPost': 1,\n",
       " 'Machine': 10,\n",
       " 'Learning': 18,\n",
       " 'Deep': 3,\n",
       " 'Other': 3,\n",
       " 'AI': 32,\n",
       " 'News': 6,\n",
       " 'Computer': 4,\n",
       " 'Vision': 4,\n",
       " 'Federated': 3,\n",
       " 'Reinforcement': 3,\n",
       " 'AutoML': 3,\n",
       " 'Startups': 3,\n",
       " 'Interview': 3,\n",
       " 'Free': 19,\n",
       " 'Courses': 3,\n",
       " 'Introduction': 3,\n",
       " 'To': 7,\n",
       " 'With': 4,\n",
       " 'Python': 6,\n",
       " 'Course': 12,\n",
       " 'For': 12,\n",
       " 'ML': 9,\n",
       " 'Maths': 3,\n",
       " 'Intro': 3,\n",
       " 'Resources': 3,\n",
       " 'Paper': 6,\n",
       " 'Summary': 5,\n",
       " 'About': 7,\n",
       " 'Us': 6,\n",
       " 'Advisory': 3,\n",
       " 'Board': 3,\n",
       " 'Members': 3,\n",
       " 'Advertise': 4,\n",
       " 'with': 10,\n",
       " 'us': 6,\n",
       " 'Search': 6,\n",
       " 'NewsWeekPRO': 2,\n",
       " 'SearchSearch': 1,\n",
       " 'Marktechpost': 7,\n",
       " 'Submit': 1,\n",
       " 'PR': 1,\n",
       " 'Startup': 3,\n",
       " 'Story': 1,\n",
       " 'Home': 1,\n",
       " 'Artificial': 2,\n",
       " 'Intelligence': 2,\n",
       " 'language': 5,\n",
       " 'processing': 1,\n",
       " 'is': 11,\n",
       " 'a': 9,\n",
       " 'branch': 1,\n",
       " 'of': 17,\n",
       " 'artificial': 1,\n",
       " 'intelligence': 1,\n",
       " 'concerned': 1,\n",
       " 'giving': 1,\n",
       " 'computers': 1,\n",
       " 'the': 25,\n",
       " 'ability': 1,\n",
       " 'to': 25,\n",
       " 'understand': 4,\n",
       " 'text': 2,\n",
       " 'and': 19,\n",
       " 'spoken': 1,\n",
       " 'words': 1,\n",
       " 'much': 1,\n",
       " 'like': 2,\n",
       " 'humans': 1,\n",
       " 'can': 1,\n",
       " 'Latest': 2,\n",
       " 'LatestFeatured': 1,\n",
       " 'postsMost': 1,\n",
       " 'popular7': 1,\n",
       " 'days': 1,\n",
       " 'popularBy': 1,\n",
       " 'review': 1,\n",
       " 'scoreRandom': 1,\n",
       " 'Microsoft': 2,\n",
       " 'Introduce': 2,\n",
       " 'AdaTest': 1,\n",
       " 'Process': 1,\n",
       " 'for': 10,\n",
       " 'Adaptive': 2,\n",
       " 'Testing': 2,\n",
       " 'Debugging': 2,\n",
       " 'Khushboo': 2,\n",
       " 'Gupta': 2,\n",
       " 'May': 6,\n",
       " '31': 1,\n",
       " '2022': 13,\n",
       " '0': 13,\n",
       " 'This': 11,\n",
       " 'Article': 6,\n",
       " 'written': 3,\n",
       " 'as': 7,\n",
       " 'summay': 3,\n",
       " 'by': 4,\n",
       " 'Staff': 1,\n",
       " 'based': 6,\n",
       " 'on': 10,\n",
       " 'Research': 6,\n",
       " 'NLP': 1,\n",
       " 'Models': 1,\n",
       " 'All': 4,\n",
       " 'Credit': 3,\n",
       " 'CoAuthor': 1,\n",
       " 'A': 7,\n",
       " 'Human': 1,\n",
       " 'Collaborative': 1,\n",
       " 'Writing': 1,\n",
       " 'Dataset': 3,\n",
       " 'Improving': 1,\n",
       " 'Tools': 1,\n",
       " 'Aneesh': 2,\n",
       " 'Tickoo': 2,\n",
       " '28': 1,\n",
       " 'Large': 3,\n",
       " 'models': 3,\n",
       " 'LMs': 1,\n",
       " 'provide': 4,\n",
       " 'novel': 1,\n",
       " 'opportunities': 1,\n",
       " 'interface': 1,\n",
       " 'design': 1,\n",
       " 'have': 4,\n",
       " 'undoubtedly': 1,\n",
       " 'advanced': 1,\n",
       " 'point': 1,\n",
       " 'where': 1,\n",
       " 'they': 2,\n",
       " 'may': 3,\n",
       " 'be': 3,\n",
       " 'compared': 1,\n",
       " 'Researchers': 6,\n",
       " 'Introduces': 2,\n",
       " 'De': 1,\n",
       " 'ToxiGen': 1,\n",
       " 'Scale': 1,\n",
       " 'Generated': 1,\n",
       " 'Adversarial': 1,\n",
       " '26': 1,\n",
       " 'While': 1,\n",
       " 'there': 2,\n",
       " 'are': 13,\n",
       " 'several': 1,\n",
       " 'benefits': 1,\n",
       " 'using': 2,\n",
       " 'also': 3,\n",
       " 'drawbacks': 1,\n",
       " 'this': 4,\n",
       " 'cutting': 1,\n",
       " 'edge': 1,\n",
       " 'technology': 1,\n",
       " 'One': 1,\n",
       " 'example': 2,\n",
       " 'creation': 1,\n",
       " 'inappropriate': 1,\n",
       " 'Google': 4,\n",
       " 'Explain': 1,\n",
       " 'The': 6,\n",
       " 'Rephrasing': 1,\n",
       " 'in': 6,\n",
       " 'Assistant': 1,\n",
       " 'Based': 4,\n",
       " 'Context': 2,\n",
       " 'Annu': 2,\n",
       " 'Kumari': 2,\n",
       " '18': 1,\n",
       " 'references': 1,\n",
       " 'drive': 1,\n",
       " 'interaction': 1,\n",
       " 'between': 1,\n",
       " 'people': 1,\n",
       " 'if': 1,\n",
       " 'asked': 2,\n",
       " 'Who': 1,\n",
       " 'wrote': 1,\n",
       " 'Romeo': 1,\n",
       " 'Juliet': 1,\n",
       " 'then': 1,\n",
       " 'Where': 1,\n",
       " 'was': 1,\n",
       " 'he': 1,\n",
       " 'born': 1,\n",
       " 'it': 1,\n",
       " 'TensorFlow': 2,\n",
       " 'New': 1,\n",
       " 'On': 5,\n",
       " 'Device': 1,\n",
       " 'Embedding': 1,\n",
       " 'Library': 2,\n",
       " 'That': 2,\n",
       " 'Allows': 1,\n",
       " 'Find': 1,\n",
       " '17': 1,\n",
       " 'Is': 2,\n",
       " 'device': 1,\n",
       " 'Text': 2,\n",
       " 'Image': 2,\n",
       " 'Lite': 1,\n",
       " 'Searcher': 1,\n",
       " 'Goes': 2,\n",
       " 'Germany': 1,\n",
       " 'Developing': 1,\n",
       " 'Next': 2,\n",
       " 'Enterprise': 1,\n",
       " 'Engine': 1,\n",
       " 'Mansi': 2,\n",
       " 'Rawat': 2,\n",
       " '14': 1,\n",
       " 'Pitch': 2,\n",
       " 'your': 11,\n",
       " 'startup': 3,\n",
       " 'story': 2,\n",
       " 'at': 3,\n",
       " 'email': 4,\n",
       " 'protected': 3,\n",
       " 'Please': 2,\n",
       " 'don': 2,\n",
       " 't': 2,\n",
       " 'forget': 2,\n",
       " 'join': 2,\n",
       " 'our': 3,\n",
       " 'Subreddit': 2,\n",
       " 'Data': 1,\n",
       " 'everywhere': 1,\n",
       " 'However': 1,\n",
       " 'having': 1,\n",
       " 'access': 1,\n",
       " 'data': 3,\n",
       " 'does': 1,\n",
       " 'not': 6,\n",
       " 'always': 1,\n",
       " 'Amazon': 4,\n",
       " 'Kickstarts': 1,\n",
       " 'Understanding': 1,\n",
       " 'By': 3,\n",
       " 'Open': 2,\n",
       " 'Sourcing': 1,\n",
       " 'MASSIVE': 1,\n",
       " 'Speech': 1,\n",
       " 'April': 4,\n",
       " '24': 1,\n",
       " 'article': 2,\n",
       " 'releases': 1,\n",
       " '51': 1,\n",
       " 'dataset': 1,\n",
       " 'understanding': 1,\n",
       " 'Meet': 1,\n",
       " 'LOVO': 1,\n",
       " 'Berkeley': 1,\n",
       " 'generation': 1,\n",
       " 'Voiceover': 1,\n",
       " '19': 1,\n",
       " 'Need': 1,\n",
       " 'help': 4,\n",
       " 'creating': 1,\n",
       " 'content': 2,\n",
       " 'lab': 1,\n",
       " 'Talk': 1,\n",
       " 'Salesforce': 1,\n",
       " 'Converse': 2,\n",
       " 'Task': 3,\n",
       " 'Oriented': 2,\n",
       " 'Dialogue': 2,\n",
       " 'System': 2,\n",
       " 'Simplifies': 1,\n",
       " 'Tanushree': 1,\n",
       " 'Shenwai': 1,\n",
       " '15': 1,\n",
       " 'summary': 1,\n",
       " 'paper': 1,\n",
       " 'Tree': 1,\n",
       " 'Modular': 1,\n",
       " 'all': 1,\n",
       " 'credit': 1,\n",
       " 'goes': 1,\n",
       " 'authors': 1,\n",
       " 'Now': 1,\n",
       " 'You': 2,\n",
       " 'Can': 1,\n",
       " 'And': 2,\n",
       " 'Images': 1,\n",
       " 'At': 2,\n",
       " 'Shruti': 1,\n",
       " '11': 1,\n",
       " 'began': 1,\n",
       " 'rolling': 1,\n",
       " 'out': 3,\n",
       " 'new': 1,\n",
       " 'tool': 1,\n",
       " 'Thursday': 1,\n",
       " 'that': 6,\n",
       " 'will': 2,\n",
       " 'allow': 1,\n",
       " 'users': 1,\n",
       " 'search': 1,\n",
       " 'information': 5,\n",
       " 'both': 1,\n",
       " 'photos': 1,\n",
       " '123': 1,\n",
       " '16Page': 1,\n",
       " '1': 4,\n",
       " '16': 1,\n",
       " 'Advertisement': 3,\n",
       " 'California': 2,\n",
       " 'Platform': 1,\n",
       " 'providing': 1,\n",
       " 'easy': 1,\n",
       " 'consume': 1,\n",
       " 'byte': 1,\n",
       " 'size': 1,\n",
       " 'updates': 1,\n",
       " 'machine': 1,\n",
       " 'learning': 2,\n",
       " 'deep': 1,\n",
       " 'science': 1,\n",
       " 'research': 1,\n",
       " 'FacebookTwitterYoutube': 1,\n",
       " 'Company': 1,\n",
       " 'Download': 1,\n",
       " 'Magazine': 1,\n",
       " 'Privacy': 2,\n",
       " 'TC': 1,\n",
       " 'Cookie': 2,\n",
       " 'Policy': 2,\n",
       " 'Industry': 1,\n",
       " 'Content': 1,\n",
       " 'Service': 1,\n",
       " 'latest': 1,\n",
       " 'From': 1,\n",
       " 'Release': 1,\n",
       " 'GRIT': 1,\n",
       " 'General': 1,\n",
       " 'Robust': 1,\n",
       " 'Benchmark': 1,\n",
       " 'Evaluating': 1,\n",
       " 'Model': 1,\n",
       " 's': 3,\n",
       " 'Performace': 1,\n",
       " 'June': 3,\n",
       " '2': 2,\n",
       " 'Most': 1,\n",
       " 'computer': 1,\n",
       " 'vision': 1,\n",
       " 'CV': 1,\n",
       " 'trained': 1,\n",
       " 'assessed': 1,\n",
       " 'NVIDIA': 1,\n",
       " 'Cambridge': 1,\n",
       " 'Supercomputer': 1,\n",
       " 'MONAI': 1,\n",
       " 'Were': 1,\n",
       " 'Utilized': 1,\n",
       " 'King': 1,\n",
       " 'College': 1,\n",
       " 'London': 1,\n",
       " 'Develop': 1,\n",
       " 'Source': 1,\n",
       " 'Synthetic': 1,\n",
       " 'Brain': 1,\n",
       " 'Pictures': 1,\n",
       " 'Which': 1,\n",
       " 'Will': 1,\n",
       " 'Help': 1,\n",
       " 'Speed': 1,\n",
       " 'Shorts': 1,\n",
       " 'In': 1,\n",
       " 'Deepmind': 1,\n",
       " 'Propose': 1,\n",
       " 'Steps': 1,\n",
       " 'Scaling': 1,\n",
       " 'Implicit': 1,\n",
       " 'Neural': 1,\n",
       " 'Representations': 1,\n",
       " 'INRs': 1,\n",
       " '2021': 1,\n",
       " 'LLC': 1,\n",
       " 'Rights': 1,\n",
       " 'Reserved': 1,\n",
       " 'Made': 1,\n",
       " 'Close': 2,\n",
       " 'module': 1,\n",
       " 'Min': 1,\n",
       " 'NewsletterJoin': 1,\n",
       " '500': 1,\n",
       " '000': 1,\n",
       " 'Professionals': 1,\n",
       " 'EmailEnter': 1,\n",
       " 'addressJoin': 1,\n",
       " 'NowNo': 1,\n",
       " 'thanks': 1,\n",
       " 'I': 1,\n",
       " 'm': 1,\n",
       " 'interested': 1,\n",
       " 'We': 2,\n",
       " 'use': 4,\n",
       " 'cookies': 22,\n",
       " 'website': 12,\n",
       " 'give': 1,\n",
       " 'you': 4,\n",
       " 'most': 1,\n",
       " 'relevant': 2,\n",
       " 'experience': 4,\n",
       " 'remembering': 1,\n",
       " 'preferences': 1,\n",
       " 'repeat': 1,\n",
       " 'visits': 1,\n",
       " 'clicking': 1,\n",
       " 'Accept': 2,\n",
       " 'consent': 3,\n",
       " 'ALL': 1,\n",
       " 'Do': 1,\n",
       " 'sell': 1,\n",
       " 'my': 1,\n",
       " 'personal': 3,\n",
       " 'settingsACCEPTPrivacy': 1,\n",
       " 'Cookies': 1,\n",
       " 'Overview': 1,\n",
       " 'uses': 1,\n",
       " 'improve': 1,\n",
       " 'while': 1,\n",
       " 'navigate': 1,\n",
       " 'through': 1,\n",
       " 'Out': 1,\n",
       " 'these': 4,\n",
       " 'categorized': 1,\n",
       " 'necessary': 4,\n",
       " 'stored': 2,\n",
       " 'browser': 2,\n",
       " 'essential': 2,\n",
       " 'working': 1,\n",
       " 'basic': 2,\n",
       " 'functionalities': 3,\n",
       " 'third': 2,\n",
       " 'party': 2,\n",
       " 'analyze': 2,\n",
       " 'how': 2,\n",
       " 'These': 4,\n",
       " 'only': 2,\n",
       " 'option': 1,\n",
       " 'opt': 1,\n",
       " 'But': 1,\n",
       " 'opting': 1,\n",
       " 'some': 1,\n",
       " 'an': 1,\n",
       " 'effect': 1,\n",
       " 'browsing': 1,\n",
       " 'Necessary': 4,\n",
       " 'Always': 1,\n",
       " 'Enabled': 1,\n",
       " 'absolutely': 1,\n",
       " 'function': 2,\n",
       " 'properly': 1,\n",
       " 'category': 2,\n",
       " 'includes': 1,\n",
       " 'ensures': 1,\n",
       " 'security': 1,\n",
       " 'features': 2,\n",
       " 'do': 1,\n",
       " 'store': 1,\n",
       " 'any': 1,\n",
       " 'Non': 1,\n",
       " 'non': 2,\n",
       " 'Any': 1,\n",
       " 'particularly': 1,\n",
       " 'used': 4,\n",
       " 'specifically': 1,\n",
       " 'collect': 3,\n",
       " 'user': 3,\n",
       " 'via': 1,\n",
       " 'analytics': 2,\n",
       " 'ads': 3,\n",
       " 'other': 2,\n",
       " 'embedded': 1,\n",
       " 'contents': 1,\n",
       " 'termed': 1,\n",
       " 'It': 1,\n",
       " 'mandatory': 1,\n",
       " 'procure': 1,\n",
       " 'prior': 1,\n",
       " 'running': 1,\n",
       " 'Analytics': 1,\n",
       " 'Analytical': 1,\n",
       " 'visitors': 5,\n",
       " 'interact': 1,\n",
       " 'metrics': 1,\n",
       " 'number': 1,\n",
       " 'bounce': 1,\n",
       " 'rate': 1,\n",
       " 'traffic': 1,\n",
       " 'source': 1,\n",
       " 'etc': 1,\n",
       " 'Performance': 2,\n",
       " 'performance': 2,\n",
       " 'key': 1,\n",
       " 'indexes': 1,\n",
       " 'which': 1,\n",
       " 'helps': 1,\n",
       " 'delivering': 1,\n",
       " 'better': 1,\n",
       " 'Uncategorized': 1,\n",
       " 'uncategorized': 1,\n",
       " 'Undefined': 1,\n",
       " 'those': 1,\n",
       " 'being': 1,\n",
       " 'analyzed': 1,\n",
       " 'been': 1,\n",
       " 'classified': 1,\n",
       " 'into': 1,\n",
       " 'yet': 1,\n",
       " 'Functional': 2,\n",
       " 'functional': 1,\n",
       " 'perform': 1,\n",
       " 'certain': 1,\n",
       " 'sharing': 1,\n",
       " 'social': 1,\n",
       " 'media': 1,\n",
       " 'platforms': 1,\n",
       " 'feedbacks': 1,\n",
       " 'advertisement': 1,\n",
       " 'marketing': 1,\n",
       " 'campaigns': 1,\n",
       " 'track': 1,\n",
       " 'across': 1,\n",
       " 'websites': 1,\n",
       " 'customized': 1,\n",
       " 'Save': 1}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs = get_freq(get_words(text))\n",
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text=re.sub(r'[0-9]','',text)\n",
    "    text=re.sub(r'[^a-zA-Z]',' ',text)\n",
    "    text=re.sub(r'\\s+',' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' natural language processing category marktechpost machine learning deep learning other ai news computer vision federated learning reinforcement learning natural language processing automl ai startups interview free ai courses free introduction to machine learning with python course free python for machine learning ml course free maths for ml course free ai intro course free ai resources ai paper summary about us about us advisory board members advertise with us search newsweekpro newsweekpro searchsearch marktechpost submit news pr startup story machine learning deep learning other ai news computer vision federated learning reinforcement learning natural language processing automl ai startups interview free ai courses free introduction to machine learning with python course free python for machine learning ml course free maths for ml course free ai intro course free ai resources ai paper summary about us about us advisory board members advertise with us marktechpost machine learning deep learning other ai news computer vision federated learning reinforcement learning natural language processing automl ai startups interview free ai courses free introduction to machine learning with python course free python for machine learning ml course free maths for ml course free ai intro course free ai resources ai paper summary about us about us advisory board members advertise with us home artificial intelligence natural language processing natural language processing natural language processing is a branch of artificial intelligence concerned with giving computers the ability to understand text and spoken words much like humans can latest latestfeatured postsmost popular days popularby review scorerandom microsoft introduce adatest a process for adaptive testing and debugging of khushboo gupta may this article is written as a summay by marktechpost staff based on the research paper adaptive testing and debugging of nlp models all credit coauthor a human ai collaborative writing dataset for improving language tools aneesh tickoo may large language models lms provide novel opportunities for interface design large language models have undoubtedly advanced to the point where they may be compared microsoft ai researchers introduces de toxigen a large scale machine generated dataset for adversarial aneesh tickoo may while there are several benefits to using artificial intelligence there are also drawbacks to this cutting edge technology one example is the creation of inappropriate google researchers explain the rephrasing in google assistant based on context annu kumari may context and references drive interaction between people for example if asked who wrote romeo and juliet and then asked where was he born it tensorflow introduces a new on device embedding based search library that allows find annu kumari may this article is based on the research article on device text to image search with tensorflow lite searcher library all credit for this research goes to this germany based ai startup is developing the next enterprise search engine mansi rawat may pitch your startup story at email protected please don t forget to join our ml subreddit data is everywhere however having access to data does not always amazon kickstarts natural language understanding by open sourcing massive speech dataset khushboo gupta april this article is based on the amazon article amazon releases language dataset for language understanding all credit for this research goes to the amazon meet lovo a berkeley based next generation ai voiceover startup mansi rawat april pitch your startup story at email protected please don t forget to join our ml subreddit need help in creating ml research content for your lab startup talk salesforce ai researchers introduce converse a task oriented dialogue system that simplifies tanushree shenwai april this summary article is based on the paper converse a tree based modular task oriented dialogue system and all credit goes to the authors of this now you can search on google with text and images at shruti april google began rolling out a new search tool on thursday that will allow users to search for information using both text and photos at page of advertisement about us marktechpost is a california based ai news platform providing easy to consume byte size updates in machine learning deep learning and data science research facebooktwitteryoutube company download ai magazine privacy tc cookie policy industry news advertise with us content service the latest ai researchers from email protected release grit a general robust image task benchmark for evaluating computer vision model s performace ai paper summary june most computer vision cv models are trained and assessed nvidia s cambridge supercomputer and monai were utilized by researchers at king s college london to develop open source synthetic brain pictures which will help to speed ai shorts june this article is written as a summay by marktechpost in the latest ai research deepmind researchers propose steps for scaling implicit neural representations inrs ai paper summary june this article is written as a summay by marktechpost marktechpost llc all rights reserved made with in california close this module free min ai newsletterjoin ai professionals emailenter your email addressjoin nowno thanks i m not interested we use cookies on our website to give you the most relevant experience by remembering your preferences and repeat visits by clicking accept you consent to the use of all the cookies do not sell my personal information cookie settingsacceptprivacy cookies policy close privacy overview this website uses cookies to improve your experience while you navigate through the website out of these cookies the cookies that are categorized as necessary are stored on your browser as they are essential for the working of basic functionalities of the website we also use third party cookies that help us analyze and understand how you use this website these cookies will be stored in your browser only with your consent you also have the option to opt out of these cookies but opting out of some of these cookies may have an effect on your browsing experience necessary necessary always enabled necessary cookies are absolutely essential for the website to function properly this category only includes cookies that ensures basic functionalities and security features of the website these cookies do not store any personal information non necessary non necessary any cookies that may not be particularly necessary for the website to function and is used specifically to collect user personal data via analytics ads other embedded contents are termed as non necessary cookies it is mandatory to procure user consent prior to running these cookies on your website analytics analytics analytical cookies are used to understand how visitors interact with the website these cookies help provide information on metrics the number of visitors bounce rate traffic source etc performance performance performance cookies are used to understand and analyze the key performance indexes of the website which helps in delivering a better user experience for the visitors uncategorized uncategorized undefined cookies are those that are being analyzed and have not been classified into a category as yet functional functional functional cookies help to perform certain functionalities like sharing the content of the website on social media platforms collect feedbacks and other third party features advertisement advertisement advertisement cookies are used to provide visitors with relevant ads and marketing campaigns these cookies track visitors across websites and collect information to provide customized ads save accept '"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a program to crawl a given web page and scrape the complete content of given URL\n",
    "\n",
    "Url=\"https://www.marktechpost.com/category/technology/artificial-intelligence/natural-language-processing/\"\n",
    "\n",
    "def start_crawl(url):\n",
    "    html_text = requests.get(url).text\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    return text\n",
    "\n",
    "def get_words(text):\n",
    "    words = re.findall(r'\\w+', text)\n",
    "    return words\n",
    "\n",
    "def content_scrape(url):\n",
    "    text = start_crawl(url)\n",
    "    return content_scrape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function content_scrape at 0x0000022BC1AF9488>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(content_scrape(Url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.00178253 0.00356506 0.00534759 0.00713012 0.00891266\n",
      " 0.01069519 0.01247772 0.01426025 0.01604278 0.01782531 0.01960784\n",
      " 0.02139037 0.02317291 0.02495544 0.02673797 0.0285205  0.03030303\n",
      " 0.03208556 0.03386809 0.03565062 0.03743316 0.03921569 0.04099822\n",
      " 0.04278075 0.04456328 0.04634581 0.04812834 0.04991087 0.0516934\n",
      " 0.05347594 0.05525847 0.057041   0.05882353]\n"
     ]
    }
   ],
   "source": [
    "#Write a program to implement The Power Method for computing Page Rank.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def power_method(G, max_iter=100, tol=1e-6):\n",
    "   \n",
    "    x = np.ones(G.number_of_nodes()) / G.number_of_nodes()\n",
    "    for _ in range(max_iter):\n",
    "        x0 = x\n",
    "        x = np.dot(G.to_directed(), x)\n",
    "        x /= x.sum()\n",
    "        if np.abs(x - x0).sum() < tol:\n",
    "            break\n",
    "    return x\n",
    "\n",
    "\n",
    "G = nx.karate_club_graph()\n",
    "x = power_method(G)\n",
    "print(x)\n",
    "\n",
    "\n",
    "Url=\"https://www.marktechpost.com/category/technology/artificial-intelligence/natural-language-processing/\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.41176471e-003 9.06618069e+194 1.81323614e+195 2.71985421e+195\n",
      " 3.62647228e+195 4.53309035e+195 5.43970841e+195 6.34632648e+195\n",
      " 7.25294455e+195 8.15956262e+195 9.06618069e+195 9.97279876e+195\n",
      " 1.08794168e+196 1.17860349e+196 1.26926530e+196 1.35992710e+196\n",
      " 1.45058891e+196 1.54125072e+196 1.63191252e+196 1.72257433e+196\n",
      " 1.81323614e+196 1.90389795e+196 1.99455975e+196 2.08522156e+196\n",
      " 2.17588337e+196 2.26654517e+196 2.35720698e+196 2.44786879e+196\n",
      " 2.53853059e+196 2.62919240e+196 2.71985421e+196 2.81051601e+196\n",
      " 2.90117782e+196 2.99183963e+196]\n"
     ]
    }
   ],
   "source": [
    "#Write a program to implement apge rank using Random Walk.\n",
    "def random_walk(G, alpha=0.85, max_iter=100, tol=1e-6):\n",
    "   \n",
    "    x = np.ones(G.number_of_nodes()) / G.number_of_nodes()\n",
    "    for _ in range(max_iter):\n",
    "        x0 = x\n",
    "        x = np.dot(G.to_directed(), x)\n",
    "        x = alpha * x + (1 - alpha) / G.number_of_nodes()\n",
    "        if np.abs(x - x0).sum() < tol:\n",
    "            break\n",
    "    return x\n",
    "\n",
    "G=nx.karate_club_graph()\n",
    "x=random_walk(G)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google\n",
      "Google\n",
      "here\n",
      "Books\n",
      "Images\n"
     ]
    }
   ],
   "source": [
    "#Write a program to display top five ranking websites of a given keyword using Page Rank Algorithm\n",
    "\n",
    "keyword=\"python programming\"\n",
    "\n",
    "\n",
    "def top_five(keyword):\n",
    "    url=\"https://www.google.com/search?q=\"+keyword+\"&num=5\"\n",
    "    html_text = requests.get(url).text\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "    links = soup.find_all('a')\n",
    "    for i in range(5):\n",
    "        print(links[i].text)\n",
    "\n",
    "top_five(keyword)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a program to retrieve pages satisfying the (Boolean) query in the web search.\n",
    "\n",
    "query=\"python is a programming language amd is used for developing web applications\"\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "\n",
    "\n",
    "def retrieve_pages(query):\n",
    "    url=\"https://www.google.com/search?q=\"+query\n",
    "    html_text = requests.get(url).text\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "    links = soup.find_all('a')\n",
    "    for i in range(5):\n",
    "        print(links[i].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google\n",
      "Google\n",
      "here\n",
      "Videos\n",
      "News\n"
     ]
    }
   ],
   "source": [
    "retrieve_pages(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "90c985cc047d5a65037cfccd0e7eba3544fde910f643ecb3dc813af6d7189d16"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('gr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
